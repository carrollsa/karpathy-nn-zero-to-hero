{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a502dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30814ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eb98857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1ef61b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "    X, Y = [], []\n",
    "  \n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9413690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af0592f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "921a5215",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a912c969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3211, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f4e1804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d3d67c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = 1.0 / probs * dlogprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum =  -counts_sum ** -2 * dcounts_sum_inv\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "dnorm_logits = counts * dcounts\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "# no need to memorize formula for following 3 -- looking at shape of dlogits, h, W2, and b2, \n",
    "# there is only one way to achieve the correct shape of output matrix for each \n",
    "# ---\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "# ---\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff += (2*bndiff) * dbndiff2\n",
    "dhprebn = dbndiff.clone()\n",
    "dbnmeani = (-dbndiff).sum(0)\n",
    "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "  for j in range(Xb.shape[1]):\n",
    "    ix = Xb[k,j]\n",
    "    dC[ix] += demb[k,j]\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9677f911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([1, 64]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.shape, bngain.shape, bnraw.shape, bnbias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ed01c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3210511207580566 diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e3ca02ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 7.2177499532699585e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23e8e04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0763, 0.0926, 0.0184, 0.0478, 0.0197, 0.0812, 0.0249, 0.0341, 0.0174,\n",
       "        0.0325, 0.0372, 0.0379, 0.0347, 0.0277, 0.0379, 0.0136, 0.0093, 0.0207,\n",
       "        0.0168, 0.0490, 0.0490, 0.0203, 0.0245, 0.0716, 0.0584, 0.0261, 0.0203],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "880aabd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0763,  0.0926,  0.0184,  0.0478,  0.0197,  0.0812,  0.0249,  0.0341,\n",
       "        -0.9826,  0.0325,  0.0372,  0.0379,  0.0347,  0.0277,  0.0379,  0.0136,\n",
       "         0.0093,  0.0207,  0.0168,  0.0490,  0.0490,  0.0203,  0.0245,  0.0716,\n",
       "         0.0584,  0.0261,  0.0203], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a02b565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8626e-09, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e0ee387d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17557bd2ef0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# each cell represents force on each character, pulling down on incorrect characters and up on correct characters\n",
    "# strength of pull is proportional to correctness of guesses -- confidently incorrectly predicted characters are \n",
    "# pulled down upon strongly, confidently correctly predicted characters will be pulled up strongly.\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c3621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
